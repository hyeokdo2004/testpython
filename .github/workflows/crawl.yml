name: Run Crawler

on:
  schedule:
    - cron: "0 18 * * *"  # ë§¤ì¼ í•œêµ­ì‹œê°„ ì˜¤ì „ 3ì‹œì— ì‹¤í–‰ (UTC+9 ê¸°ì¤€)
  workflow_dispatch:       # ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥í•˜ê²Œ í•¨

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      # 1ï¸âƒ£ ë¦¬í¬ì§€í† ë¦¬ ì²´í¬ì•„ì›ƒ
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2ï¸âƒ£ Python í™˜ê²½ ì„¤ì •
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 3ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml

      # 4ï¸âƒ£ í¬ë¡¤ëŸ¬ ì‹¤í–‰
      - name: Run crawler
        run: |
          python crawler.py

      # 5ï¸âƒ£ ê²°ê³¼ë¬¼ í™•ì¸ ë° ì»¤ë°‹ ì¤€ë¹„
      - name: Commit results
        run: |
          mkdir -p docs
          if [ -f collected_urls.txt ]; then
            mv collected_urls.txt docs/collected_urls.txt
            git config --global user.name "github-actions[bot]"
            git config --global user.email "github-actions[bot]@users.noreply.github.com"
            git add docs/collected_urls.txt
            git commit -m "ğŸ•·ï¸ Auto-update collected URLs [$(date '+%Y-%m-%d %H:%M:%S')]"
            git push
          else
            echo "âš ï¸ collected_urls.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
          fi
