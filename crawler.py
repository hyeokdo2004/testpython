# git.py
import asyncio
from playwright.async_api import async_playwright
import re

BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]
BASE_DOMAIN = "https://www.koref.or.kr"
LIST_TPL = BASE_DOMAIN + "/web/board/boardContentsListPage.do?board_id={}&miv_pageNo={}"
DETAIL_TPL = BASE_DOMAIN + "/web/board/boardContentsView.do?board_id={}&contents_id={}"

RE_CONTENTS = re.compile(r"contentsView\(['\"]?([0-9a-fA-F]+)['\"]?\)")

async def crawl_board(page, board_id: int):
    print("\n" + "=" * 30)
    print(f"ğŸ“ ê²Œì‹œíŒ board_id={board_id} ì‹œì‘")
    print("=" * 30)

    # 1ï¸âƒ£ ì²« í˜ì´ì§€ ì ‘ì†
    first_url = LIST_TPL.format(board_id, 1)
    await page.goto(first_url, timeout=60000)
    await page.wait_for_load_state("networkidle")
    await asyncio.sleep(1.5)

    # 2ï¸âƒ£ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ íƒìƒ‰
    last_page = 1
    try:
        last_img = await page.query_selector("img[alt='ë§¨ë’¤ë¡œ']")
        if last_img:
            parent_a = await last_img.evaluate_handle("node => node.closest('a')")
            href = await parent_a.get_attribute("href")
            if href and "go_Page" in href:
                m = re.search(r"go_Page\((\d+)\)", href)
                if m:
                    last_page = int(m.group(1))
    except Exception as e:
        print(" [WARN] ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ì¤‘ ì˜ˆì™¸:", e)

    print(f"[INFO] ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸: {last_page}")

    # 3ï¸âƒ£ ê° í˜ì´ì§€ ë°˜ë³µ
    for p in range(1, last_page + 1):
        page_url = LIST_TPL.format(board_id, p)
        print(f"\n--- ğŸ“„ í˜ì´ì§€ {p} â†’ {page_url}")

        if p > 1:
            try:
                await page.evaluate(f"go_Page({p})")
            except:
                await page.goto(page_url, timeout=60000)

        await page.wait_for_load_state("networkidle")
        await asyncio.sleep(1.5)

        # 4ï¸âƒ£ í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ë§í¬ ìˆ˜ì§‘ (href + onclick)
        link_values = await page.eval_on_selector_all(
            "a",
            """els => els.map(a => ({
                href: a.getAttribute('href') || '',
                onclick: a.getAttribute('onclick') || '',
                text: (a.innerText || '').trim()
            }))"""
        )

        found_any = False

        # 5ï¸âƒ£ ê²Œì‹œë¬¼ URL ì‹ë³„ ë° ì²˜ë¦¬
        for item in link_values:
            href = item.get("href", "")
            onclick = item.get("onclick", "")
            joined = href + " " + onclick
            m = RE_CONTENTS.search(joined)
            if not m:
                continue

            found_any = True
            contents_id = m.group(1)
            detail_url = DETAIL_TPL.format(board_id, contents_id)
            print(f" ğŸ“° ê²Œì‹œë¬¼ URL: {detail_url}")

            # 6ï¸âƒ£ ìƒì„¸í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            try:
                detail_page = await page.context.new_page()
                await detail_page.goto(detail_url, timeout=60000)
                await detail_page.wait_for_load_state("networkidle")
                await asyncio.sleep(1.2)

                file_links = await detail_page.eval_on_selector_all(
                    "a[href*='fileidDownLoad'], dd.vdd.file a",
                    "els => els.map(a => a.getAttribute('href'))"
                )

                for fh in file_links:
                    if not fh:
                        continue
                    full = fh if fh.startswith("http") else (BASE_DOMAIN + fh)
                    print(f" â””â”€â”€ ğŸ“ ì²¨ë¶€íŒŒì¼: {full}")

                await detail_page.close()
            except Exception as e:
                print(f" âš ï¸ ìƒì„¸í˜ì´ì§€ ì ‘ê·¼/ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                try:
                    await detail_page.close()
                except:
                    pass

        if not found_any:
            print(" âš ï¸ ì´ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í•¨ (ì •ì  ë Œë”ë§ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")

async def main():
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()
        print("\n=== ì‹œì‘: ëª¨ë“  board_id ê²Œì‹œíŒ ìˆ˜ì§‘ ===")

        for bid in BOARD_IDS:
            await crawl_board(page, bid)

        print("\n=== ì™„ë£Œ ===")
        await context.close()
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
