# crawler.py
import asyncio
from playwright.async_api import async_playwright
import re
import os

# ê²Œì‹œíŒ ID ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ ê¸°ì¤€ ì „ì²´)
BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]

BASE_DOMAIN = "https://www.koref.or.kr"
LIST_TPL = BASE_DOMAIN + "/web/board/boardContentsListPage.do?board_id={}&miv_pageNo={}"
DETAIL_TPL = BASE_DOMAIN + "/web/board/boardContentsView.do?board_id={}&contents_id={}"

# regex to extract contents_id from javascript call like contentsView('...')
RE_CONTENTS = re.compile(r"contentsView\(['\"]?([0-9a-fA-F]+)['\"]?\)")

OUTPUT_DIR = "docs"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "collected_urls.txt")
os.makedirs(OUTPUT_DIR, exist_ok=True)

collected_urls = []

async def crawl_board(page, board_id: int):
    print("\n" + "="*30)
    print(f"ğŸ“ ê²Œì‹œíŒ board_id={board_id} ì‹œì‘")
    print("="*30)

    first_url = LIST_TPL.format(board_id, 1)
    await page.goto(first_url, timeout=60000)
    await page.wait_for_load_state("networkidle")

    last_page = 1
    try:
        last_img = await page.query_selector("img[alt='ë§¨ë’¤ë¡œ']")
        if last_img:
            parent_a = await last_img.evaluate_handle("node => node.closest('a')")
            href = await parent_a.get_attribute("href")
            if href and "go_Page" in href:
                m = re.search(r"go_Page\((\d+)\)", href)
                if m:
                    last_page = int(m.group(1))
    except Exception as e:
        print(" [WARN] ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ì¤‘ ì˜ˆì™¸:", e)

    print(f"[INFO] ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸: {last_page}")

    for p in range(1, last_page + 1):
        page_url = LIST_TPL.format(board_id, p)
        print(f"\n--- ğŸ“„ í˜ì´ì§€ {p} â†’ {page_url}")
        await page.goto(page_url, timeout=60000)
        await page.wait_for_load_state("networkidle")
        await asyncio.sleep(0.4)

        # ê²Œì‹œë¬¼ ë§í¬
        link_values = await page.eval_on_selector_all(
            "a",
            """els => els.map(a => ({
                href: a.getAttribute('href') || '',
                onclick: a.getAttribute('onclick') || '',
                text: (a.innerText || '').trim()
            }))"""
        )

        found_any = False
        for item in link_values:
            href = item.get("href", "")
            onclick = item.get("onclick", "")
            joined = href + " " + onclick

            m = RE_CONTENTS.search(joined)
            if not m:
                continue
            found_any = True
            contents_id = m.group(1)
            detail_url = DETAIL_TPL.format(board_id, contents_id)
            print(f"  ğŸ“° ê²Œì‹œë¬¼ URL: {detail_url}")
            collected_urls.append(detail_url)

            # ìƒì„¸ í˜ì´ì§€ ì²¨ë¶€íŒŒì¼
            try:
                detail_page = await page.context.new_page()
                await detail_page.goto(detail_url, timeout=60000)
                await detail_page.wait_for_load_state("networkidle")

                file_links = await detail_page.eval_on_selector_all(
                    "dd.vdd.file a[href*='fileidDownLoad'], a[href*='fileidDownLoad']",
                    "els => els.map(a => a.getAttribute('href'))"
                )
                for fh in file_links:
                    if not fh:
                        continue
                    full = fh if fh.startswith("http") else (BASE_DOMAIN + fh)
                    print(f"     â””â”€â”€ ğŸ“ ì²¨ë¶€íŒŒì¼: {full}")
                    collected_urls.append(full)

                await detail_page.close()
            except Exception as e:
                print(f"     âš ï¸ ìƒì„¸í˜ì´ì§€ ì ‘ê·¼/ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                try:
                    await detail_page.close()
                except:
                    pass

        if not found_any:
            print(" âš ï¸ ì´ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í•¨ (ë Œë”ë§ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")

async def main():
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        print("\n=== ì‹œì‘: ëª¨ë“  board_id ê²Œì‹œíŒ ìˆ˜ì§‘ ===")
        for bid in BOARD_IDS:
            await crawl_board(page, bid)
        print("\n=== ì™„ë£Œ ===")

        # ì €ì¥
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            for url in collected_urls:
                f.write(url + "\n")
        print(f"ğŸ“ ì´ {len(collected_urls)}ê°œ URL ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")

        await context.close()
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
