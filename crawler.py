# crawler.py
import asyncio
from playwright.async_api import async_playwright
import re
import os

# ì²« ë²ˆì§¸ ê²Œì‹œíŒ URL
BOARD_URL = "https://www.koref.or.kr/web/board/boardContentsListPage.do?board_id=27"

# contents_id ì¶”ì¶œìš© ì •ê·œì‹
RE_CONTENTS = re.compile(r"contentsView\(['\"]?([0-9a-fA-F]+)['\"]?\)")

# ì¶œë ¥ íŒŒì¼
OUTPUT_FILE = os.path.join("docs", "collected_contents_ids.txt")
os.makedirs("docs", exist_ok=True)

async def main():
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        print(f"ğŸ”— í˜ì´ì§€ ì ‘ì†: {BOARD_URL}")
        await page.goto(BOARD_URL, timeout=60000)
        await page.wait_for_load_state("networkidle")

        # í˜ì´ì§€ ë‚´ a íƒœê·¸ onclick ì†ì„± ì¶”ì¶œ
        link_data = await page.eval_on_selector_all(
            "a",
            """els => els.map(a => a.getAttribute('onclick') || '')"""
        )

        contents_ids = set()
        for onclick in link_data:
            m = RE_CONTENTS.search(onclick)
            if m:
                contents_ids.add(m.group(1))

        # ê²°ê³¼ ì €ì¥
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            for cid in sorted(contents_ids):
                f.write(cid + "\n")
                print(f"ğŸ“ ë°œê²¬: {cid}")

        print(f"\nâœ… ì´ {len(contents_ids)}ê°œ contents_id ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")

        await context.close()
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
