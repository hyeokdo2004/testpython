# crawler.py
import asyncio
from playwright.async_api import async_playwright
import os

BASE_DOMAIN = "https://www.koref.or.kr"
BOARD_ID = 27
OUTPUT_FILE = os.path.join("docs", "collected_contents_ids.txt")
os.makedirs("docs", exist_ok=True)

async def main():
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        contents_ids = []

        # ì‘ë‹µ ì´ë²¤íŠ¸ ì²˜ë¦¬
        async def handle_response(response):
            if "boardContentsList.do" in response.url and response.status == 200:
                try:
                    data = await response.json()
                    for item in data.get("boardList", []):
                        cid = item.get("contents_id")
                        if cid:
                            contents_ids.append(cid)
                except Exception as e:
                    print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:", e)

        page.on("response", handle_response)

        list_url = f"{BASE_DOMAIN}/web/board/boardContentsListPage.do?board_id={BOARD_ID}"
        print(f"ğŸ”— í˜ì´ì§€ ì ‘ì†: {list_url}")
        await page.goto(list_url, timeout=60000)

        # AJAX ë¡œë”© ì‹œê°„ ëŒ€ê¸°
        await asyncio.sleep(3)

        # ê²°ê³¼ ì €ì¥
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            for cid in contents_ids:
                f.write(cid + "\n")

        print(f"âœ… ì´ {len(contents_ids)}ê°œ contents_id ì¶”ì¶œ")
        print(f"âœ… ê²°ê³¼ ì €ì¥ â†’ {OUTPUT_FILE}")

        await context.close()
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
