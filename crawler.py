# crawler.py
import asyncio
import requests
from playwright.async_api import async_playwright
import os

# ê²Œì‹œíŒ ID ë¦¬ìŠ¤íŠ¸
BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]

BASE_DOMAIN = "https://www.koref.or.kr"
LIST_PAGE_URL = BASE_DOMAIN + "/web/board/boardContentsListPage.do?board_id={}"
LIST_API_URL  = BASE_DOMAIN + "/web/board/boardContentsList.do"
DETAIL_URL    = BASE_DOMAIN + "/web/board/boardContentsView.do?board_id={}&contents_id={}"
FILE_URL      = BASE_DOMAIN + "/web/board/fileDownload.do?file_id={}"

OUTPUT_FILE = os.path.join("docs", "collected_urls.txt")
os.makedirs("docs", exist_ok=True)

async def get_session_cookies():
    """Playwrightë¡œ ì²« í˜ì´ì§€ ì ‘ì† í›„ ì„¸ì…˜ ì¿ í‚¤ íšë“"""
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto(LIST_PAGE_URL.format(BOARD_IDS[0]), timeout=60000)
        await page.wait_for_load_state("networkidle")

        cookies = await context.cookies()
        session_cookies = {c['name']: c['value'] for c in cookies}
        await context.close()
        await browser.close()
        print("âœ… Playwrightë¡œ ì„¸ì…˜ ì¿ í‚¤ í™•ë³´:", session_cookies)
        return session_cookies

def fetch_board_list(session: requests.Session, board_id: int, page_no: int = 1):
    """requestsë¡œ ê²Œì‹œë¬¼ ëª©ë¡ ìš”ì²­"""
    headers = {
        "User-Agent": "Mozilla/5.0",
        "X-Requested-With": "XMLHttpRequest",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": LIST_PAGE_URL.format(board_id)
    }

    data = {
        "board_id": board_id,
        "miv_pageNo": page_no,
        # í•„ìš”ì‹œ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    }

    resp = session.post(LIST_API_URL, headers=headers, data=data)
    resp.raise_for_status()
    return resp.json().get("boardList", [])

async def main():
    # 1ï¸âƒ£ Playwrightë¡œ ì„¸ì…˜ ì¿ í‚¤ í™•ë³´
    cookies = await get_session_cookies()

    # 2ï¸âƒ£ requests ì„¸ì…˜ì— ì¿ í‚¤ ì ìš©
    session = requests.Session()
    for k, v in cookies.items():
        session.cookies.set(k, v, domain="www.koref.or.kr")

    # 3ï¸âƒ£ ê²Œì‹œíŒ ìˆœíšŒ
    total_count = 0
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for board_id in BOARD_IDS:
            print(f"\nğŸ“ ê²Œì‹œíŒ board_id={board_id} ìˆ˜ì§‘ ì‹œì‘")
            try:
                board_list = fetch_board_list(session, board_id, page_no=1)
            except Exception as e:
                print(f" âš ï¸ board_id={board_id}, page=1 ìš”ì²­/íŒŒì‹± ì˜¤ë¥˜:", e)
                continue

            if not board_list:
                print(f" âš ï¸ board_id={board_id}, ê²Œì‹œë¬¼ì´ ì—†ìŒ")
                continue

            for board in board_list:
                b_id = board.get("board_id")
                contents_id = board.get("contents_id")
                file_id = board.get("file_id")

                detail_url = DETAIL_URL.format(b_id, contents_id)
                f.write(detail_url + "\n")
                total_count += 1
                print(" ğŸ“° ê²Œì‹œë¬¼ URL:", detail_url)

                if file_id:
                    file_url = FILE_URL.format(file_id)
                    f.write(file_url + "\n")
                    print(" â””â”€â”€ ğŸ“ ì²¨ë¶€íŒŒì¼ URL:", file_url)

    print(f"\nğŸ“ ì´ {total_count}ê°œ ê²Œì‹œë¬¼ URLê³¼ ì²¨ë¶€íŒŒì¼ URL ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")

if __name__ == "__main__":
    asyncio.run(main())
