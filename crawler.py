# crawler_github.py
import requests
import os

BASE_DOMAIN = "https://www.koref.or.kr"
BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]

OUTPUT_FILE = os.path.join("docs", "urls.txt")
os.makedirs("docs", exist_ok=True)

session = requests.Session()
headers = {
    "User-Agent": "Mozilla/5.0",
    "X-Requested-With": "XMLHttpRequest",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
}

total_count = 0

for board_id in BOARD_IDS:
    print(f"\nğŸ“ ê²Œì‹œíŒ board_id={board_id} ìˆ˜ì§‘ ì‹œì‘")
    data = {
        "board_id": board_id,
        "miv_pageNo": 1,
        # í•„ìš”ì‹œ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë„ ì—¬ê¸°ì— ë„£ê¸°
    }

    resp = session.post(f"{BASE_DOMAIN}/web/board/boardContentsList.do", headers=headers, data=data)
    try:
        json_data = resp.json()
    except Exception as e:
        print(f"âš ï¸ board_id={board_id}: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}")
        continue

    board_list = json_data.get("boardList", [])
    if not board_list:
        print(f"âš ï¸ board_id={board_id}: ê²Œì‹œë¬¼ì´ ì—†ìŒ")
        continue

    with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
        for board in board_list:
            contents_id = board.get("contents_id")
            file_id = board.get("file_id")

            # ê²Œì‹œë¬¼ ìƒì„¸ URL
            detail_url = f"{BASE_DOMAIN}/web/board/boardContentsView.do?board_id={board_id}&contents_id={contents_id}"
            f.write(detail_url + "\n")
            total_count += 1

            # ì²¨ë¶€íŒŒì¼ URL
            if file_id:
                file_url = f"{BASE_DOMAIN}/web/board/fileDownload.do?file_id={file_id}"
                f.write(file_url + "\n")

print(f"\nğŸ“ ì´ {total_count}ê°œ ê²Œì‹œë¬¼ URLê³¼ ì²¨ë¶€íŒŒì¼ URL ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")
