import asyncio
from playwright.async_api import async_playwright
import re
import json
import os

# ìˆ˜ì§‘ ëŒ€ìƒ ê²Œì‹œíŒ
BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]
BASE_LIST_URL = "https://www.koref.or.kr/web/board/boardContentsListPage.do?board_id={bid}&miv_pageNo={page}"
BASE_DETAIL_URL = "https://www.koref.or.kr/web/board/boardContentsView.do?board_id={bid}&contents_id={contents_id}"
BASE_DOMAIN = "https://www.koref.or.kr"

# ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
collected_urls = []

async def crawl_board(board_id, page):
    print("\n" + "="*30)
    print(f"ğŸ“ ê²Œì‹œíŒ board_id={board_id} ì‹œì‘")
    print("="*30)

    # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
    await page.goto(BASE_LIST_URL.format(bid=board_id, page=1), timeout=0)
    await page.wait_for_load_state("networkidle")
    html = await page.content()

    match = re.search(r'go_Page\((\d+)\)[^>]*>\s*<img[^>]+alt="ë§¨ë’¤ë¡œ"', html)
    max_page = int(match.group(1)) if match else 1
    print(f"[INFO] ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸: {max_page}")

    # í˜ì´ì§€ ìˆœíšŒ
    for p in range(1, max_page + 1):
        list_url = BASE_LIST_URL.format(bid=board_id, page=p)
        print(f"\n--- ğŸ“„ í˜ì´ì§€ {p} â†’ {list_url} ---")
        await page.goto(list_url, timeout=0)
        await page.wait_for_load_state("networkidle")

        anchors = await page.query_selector_all("a[href^='javascript:contentsView']")
        if not anchors:
            print(f"âš ï¸ {p} í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
            continue

        for a in anchors:
            href = await a.get_attribute("href") or ""
            match = re.search(r"contentsView\(['\"]?([0-9a-fA-F]+)['\"]?\)", href)
            if not match:
                continue

            contents_id = match.group(1)
            detail_url = BASE_DETAIL_URL.format(bid=board_id, contents_id=contents_id)
            print(f"  ğŸ“° ê²Œì‹œë¬¼ URL: {detail_url}")
            collected_urls.append(detail_url)

            # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
            await page.goto(detail_url, timeout=0)
            await page.wait_for_load_state("networkidle")

            # ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ì¶œ
            attach_links = await page.query_selector_all("dd.vdd.file a[href*='fileidDownLoad']")
            for link in attach_links:
                file_href = await link.get_attribute("href") or ""
                file_url = BASE_DOMAIN + file_href if file_href.startswith("/") else file_href
                print(f"     â””â”€â”€ ğŸ“ ì²¨ë¶€íŒŒì¼: {file_url}")
                collected_urls.append(file_url)

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        for bid in BOARD_IDS:
            await crawl_board(bid, page)

        await browser.close()

    # ê²°ê³¼ë¥¼ docs í´ë”ì— ì €ì¥
    os.makedirs("docs", exist_ok=True)
    with open("docs/result_urls.json", "w", encoding="utf-8") as f:
        json.dump(collected_urls, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(collected_urls)}ê°œ URLì´ docs/result_urls.jsonì— ì €ì¥ë¨.")

if __name__ == "__main__":
    asyncio.run(main())
