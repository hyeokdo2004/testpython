import requests
from bs4 import BeautifulSoup
import os
import time

BASE_URL = "https://www.koref.or.kr"
LIST_URL = f"{BASE_URL}/web/board/boardContentsList.do"

# ğŸ‘‡ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ê²Œì‹œíŒ ID ì „ì²´
BOARD_IDS = [27, 49, 28, 29, 30, 50, 51, 52, 39, 37, 32]

# docs í´ë”ì— ê²°ê³¼ ì €ì¥ (GitHub Pagesì—ì„œ ë°”ë¡œ ì ‘ê·¼ ê°€ëŠ¥)
os.makedirs("docs", exist_ok=True)
output_path = os.path.join("docs", "collected_urls.txt")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "X-Requested-With": "XMLHttpRequest",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Origin": BASE_URL,
    "Referer": BASE_URL + "/web/board/boardContentsListPage.do",
}

def crawl_board(board_id, max_pages=5):
    all_links = []
    print(f"\n==============================")
    print(f"ğŸ“ ê²Œì‹œíŒ board_id={board_id} ì‹œì‘")
    print(f"==============================")

    for page_no in range(1, max_pages + 1):
        payload = {
            "board_id": board_id,
            "miv_pageNo": page_no
        }

        res = requests.post(LIST_URL, headers=HEADERS, data=payload)
        res.encoding = "utf-8"

        if res.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        rows = soup.select("ul.boardList li a")

        if not rows:
            print(f"âš ï¸ í˜ì´ì§€ {page_no}: ê²Œì‹œë¬¼ ì—†ìŒ (ë” ì´ìƒ ì—†ìŒ)")
            break

        print(f"ğŸ“„ í˜ì´ì§€ {page_no} â†’ ê²Œì‹œë¬¼ {len(rows)}ê°œ")

        for a in rows:
            href = a.get("href")
            title = a.get_text(strip=True)
            if href and "javascript" not in href:
                full_url = href if href.startswith("http") else BASE_URL + href
                all_links.append((title, full_url))

        time.sleep(0.5)

    return all_links

def main():
    all_results = []
    for bid in BOARD_IDS:
        board_links = crawl_board(bid, max_pages=20)
        all_results.extend(board_links)

    if not all_results:
        print("âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì—†ìŒ.")
        return

    with open(output_path, "w", encoding="utf-8") as f:
        for title, url in all_results:
            f.write(f"<a href=\"{url}\">{title}</a>\n")

    print(f"\nâœ… ì™„ë£Œ: ì´ {len(all_results)}ê°œ URL ì €ì¥ë¨ â†’ {output_path}")

if __name__ == "__main__":
    main()
